transformer_block_type = 'PreLN'
wandb_run_name = 'PreLN-dr_0.1'
out_dir = 'analysis/size_25M/PreLN-dr_0.1'
wandb_project = 'SASP-25M-analysis'
dataset = 'shakespeare_char'
eval_interval = 200
eval_iters = 100
log_interval = 10
always_save_checkpoint = False
wandb_log = True
gradient_accumulation_steps = 1
batch_size = 128
block_size = 256
n_layer = 8
n_head = 8
learning_rate = 0.2e-3
max_iters = 25000
lr_decay_iters = 25000
min_lr = 0.2e-4
beta2 = 0.99
warmup_iters = 2000
dropout = 0.1
use_v = True
n_embd = 520
