## Objective
Re-host Lamma2 in a different arrangement of the transformer architecture; investigate signal propagation claims; investigate system identification issues with attention mechanisms. 

## Notes

- rand vs. randn probing
- llama2 has a nonstandard mlp.  That's implemented here for comparison.  Does not seem to matter which version is used, no transfer training effects observed (perhaps weakly in favor of standard ove llama2).
- 
