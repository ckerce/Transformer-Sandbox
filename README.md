## Transformer Sandbox

I forked Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT) to create an easy-to-use analysis environment for experimenting with transformer architectures.  nanoGPT is a simple and fast repository for training/finetuning medium-sized GPTs.  The local instructions for settup for this repo are found [here](docs/nanoGPT-README.md). 

I'm preparing baseline material for jumping off points for LLM and Generative AI investitation.  Current topics of interest are the following:
* [Flexible Transfomer Architecture](docs/simplified-transformers_README.md):  Investigation of observations and assertions from the paper [Simplified Transformer Blocks](https://arxiv.org/abs/2311.01906).
* Signal Propgation Analysis: `TODO` 
* Simplified Transformer w/ ALiBi: `TODO` 
* Mixture of Experts - Simplified Transformer: `TODO`
* Diffusion Language Models: `TODO`

